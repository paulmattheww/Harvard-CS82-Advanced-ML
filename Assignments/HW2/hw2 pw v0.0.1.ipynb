{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sept 16, 2018 Trend Detection CSCI E-82 Homework 2\n",
    "### Due: October 1, 2018 11:59pm EST\n",
    "\n",
    "## Overview\n",
    "\n",
    "***Identifying technology trends is of core importance to venture capitalists, companies and individuals who may invest money or time to pursue the hottest areas. Using historic data, the goals are to characterize either an increase or decrease in certain areas over a span of time, and use that information to predict the next areas before everyone becomes aware of the trend. Economists and financial traders routinely develop methods to achieve this goal using numeric data, but that’s a different problem.***\n",
    "\n",
    "Mining published literature for trend detection is not a new area, but it is far from being adequately solved. There are a number of papers that describe case studies for a given area, but none offer a definitive approach; most focus on only a niche area. The two main approaches to the problem are using word concepts and citation networks. The word concept approach aims to characterize a subfield by its component terms automatically and then look for patterns over time. Google Trends offers a plot of word frequency over time, but subfields tend to be more complex in that “convolutional neural network” has synonyms or abbreviations (CNN) that can be ambiguous. Furthermore, as areas mature, the concepts may refine into distinct groups and associate with specific sets of terms. The citation network looks for patterns in which authors are referenced to characterize concepts. These can be used to separate different areas based on which paper is cited, but also tend to be fairly noisy.\n",
    "This homework will give you and a required partner a chance to develop your text mining skills to computationally find the top 10 upward or downward trending areas within the context of 30 years of the Neural Information Processing Systems (NIPS) proceedings for their annual conference.\n",
    "\n",
    "## Data Set\n",
    "The official data set is the NIPS Proceedings available at https://papers.nips.cc/. However, this will take a long time to download and hammer their server so we will would like to provide you with alternatives. There is a version of the dataset here: https://www.kaggle.com/benhamner/nips-papers. You will need a Kaggle login in order to download it. Since I would prefer everyone spend more time on the analysis and less time on the cleaning, I am working to put out a slightly cleaner version of the official data set shortly that I will post.\n",
    "Partners:\n",
    "  HW2 is a partnered homework so work should be completed with 1 partner. To help everyone find a partner, we ask you to sign up by putting your partner's first name next to yours and vice versa using this shared spreadsheet: https://docs.google.com/spreadsheets/d/1oz0pNYx8X2WptwiLsD9zMUtsCVZiEUTFXZ5DEnPaewk/edit?u\n",
    " sp=sharing. This will give everyone immediate feedback on who doesn't have a partner.\n",
    "To select a partner, the self-intros on piazza are a good place to start. Please use the Canvas email to contact them since we respect your privacy and don't want to post everyone's email.\n",
    "  \n",
    "## Suggestions on Strategies\n",
    "You are welcome to pursue any approach. If you find applicable methods online, feel free to use them and be sure to cite the results. I would recommend starting with the text mining pipeline described in lecture and section to clean the documents and identify single- and perhaps multi-word terms. In this case, the first pass might be to perform simple counting as a baseline over time and work for a standard approach to plot trends taking the normalization into account. In the next pass, you might expand from the isolated word terms to synonyms to larger concept subfields that may cluster together. The citations or co-related words can be helpful for this. Further refinements might be to include only certain sections of the documents or try weighting schemes.\n",
    "\n",
    "## Grading Philosophy\n",
    "We will grade based on 1) your success in the project so label your final result, and 2) your exploration of different ideas. Please document your success, but also document your rationale and failed approaches. We want to know which hypotheses you pursued and how they panned out. With these kinds of homework, we expect both partners to work together and contribute equally to a greater result than either could do alone given the time constraints. We will post a form to assess your partner’s contribution relative to yours.\n",
    "\n",
    "## What to Submit\n",
    "Please submit your python notebook and associated pdf of that notebook. In a separate document, please also submit a brief description (1-2 paragraphs each) as a separate document to address the following:\n",
    "1. How have you defined a trend? How can you separate it from background noise and/or spurious relationships?\n",
    "2. What are the main techniques you have used and how have you tailored them for this problem?\n",
    "3. What was your strategy for finding multi-word phrases versus single words?\n",
    "4. What approach(es) did you use to separate one subfield from others?\n",
    "5. What parts of the document did you use and why?\n",
    "6. How did you normalize the results against the growth of the conference, lengths of documents, etc.?\n",
    "7. We know that you can look back and find trends but how would you find the next trend with your method? Be specific.\n",
    "8. Plot of the final top 10 normalized trends as a function of time.\n",
    "\n",
    "To assist the grading within the notebook:\n",
    "\n",
    "* Label your final approach within the file for grading purposes.\n",
    "* Flag the distinct approaches with a header describing your strategy and corresponding results.\n",
    "\n",
    "It makes it much easier to follow your rationale with headers and descriptions than trying guess using the code alone.\n",
    "We hope that you find this to be an interesting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "import spacy\n",
    "from tld import get_tld\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors table:\n",
      " **************************************************\n",
      "There are 9784 unique authors\n",
      "Shape of authors table (9784, 1) \n",
      "\n",
      "Paper Author table:\n",
      " **************************************************\n",
      "There exist 7238 papers authored by 9784 unique authors\n",
      "There are 20823 unique author/paper pairs\n",
      "Shape of Paper Author table (20838, 3)\n",
      "\n",
      "Papers table:\n",
      " **************************************************\n",
      "There exist 7241 papers with 7241 unique titles\n",
      "Shape of Papers table (7241, 6)\n",
      "\n",
      "Combining data to one structure\n",
      " **************************************************\n",
      "Mapping author names to the Paper Authors table in preparation to merge\n",
      "After merging there are 3 missing values in the author field\n",
      "Final shape of data (20841, 10)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>paper_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>94.0</td>\n",
       "      <td>63_94</td>\n",
       "      <td>Yaser S. Abu-Mostafa</td>\n",
       "      <td>1987</td>\n",
       "      <td>Connectivity Versus Entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63-connectivity-versus-entropy.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>1\\n\\nCONNECTIVITY VERSUS ENTROPY\\nYaser S. Abu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>124.0</td>\n",
       "      <td>80_124</td>\n",
       "      <td>Joshua Alspector</td>\n",
       "      <td>1987</td>\n",
       "      <td>Stochastic Learning Networks and their Electro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80-stochastic-learning-networks-and-their-elec...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>9\\n\\nStochastic Learning Networks and their El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>125.0</td>\n",
       "      <td>80_125</td>\n",
       "      <td>Robert B. Allen</td>\n",
       "      <td>1987</td>\n",
       "      <td>Stochastic Learning Networks and their Electro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80-stochastic-learning-networks-and-their-elec...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>9\\n\\nStochastic Learning Networks and their El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>126.0</td>\n",
       "      <td>80_126</td>\n",
       "      <td>Victor Hu</td>\n",
       "      <td>1987</td>\n",
       "      <td>Stochastic Learning Networks and their Electro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80-stochastic-learning-networks-and-their-elec...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>9\\n\\nStochastic Learning Networks and their El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>127.0</td>\n",
       "      <td>80_127</td>\n",
       "      <td>Srinagesh Satyanarayana</td>\n",
       "      <td>1987</td>\n",
       "      <td>Stochastic Learning Networks and their Electro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80-stochastic-learning-networks-and-their-elec...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>9\\n\\nStochastic Learning Networks and their El...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  author_id paper_author_id                   author  year  \\\n",
       "0        63       94.0           63_94     Yaser S. Abu-Mostafa  1987   \n",
       "1        80      124.0          80_124         Joshua Alspector  1987   \n",
       "2        80      125.0          80_125          Robert B. Allen  1987   \n",
       "3        80      126.0          80_126                Victor Hu  1987   \n",
       "4        80      127.0          80_127  Srinagesh Satyanarayana  1987   \n",
       "\n",
       "                                               title event_type  \\\n",
       "0                        Connectivity Versus Entropy        NaN   \n",
       "1  Stochastic Learning Networks and their Electro...        NaN   \n",
       "2  Stochastic Learning Networks and their Electro...        NaN   \n",
       "3  Stochastic Learning Networks and their Electro...        NaN   \n",
       "4  Stochastic Learning Networks and their Electro...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0                 63-connectivity-versus-entropy.pdf  Abstract Missing   \n",
       "1  80-stochastic-learning-networks-and-their-elec...  Abstract Missing   \n",
       "2  80-stochastic-learning-networks-and-their-elec...  Abstract Missing   \n",
       "3  80-stochastic-learning-networks-and-their-elec...  Abstract Missing   \n",
       "4  80-stochastic-learning-networks-and-their-elec...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  1\\n\\nCONNECTIVITY VERSUS ENTROPY\\nYaser S. Abu...  \n",
       "1  9\\n\\nStochastic Learning Networks and their El...  \n",
       "2  9\\n\\nStochastic Learning Networks and their El...  \n",
       "3  9\\n\\nStochastic Learning Networks and their El...  \n",
       "4  9\\n\\nStochastic Learning Networks and their El...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in authors table\n",
    "authors = pd.read_csv('data/authors.csv')\n",
    "authors.set_index('id', drop=True, inplace=True)\n",
    "authors.index.name = 'author_id'\n",
    "\n",
    "# print details of authors table\n",
    "print('Authors table:\\n', '*'*50)\n",
    "print('There are %i unique authors' %authors.index.nunique())\n",
    "print('Shape of authors table %s \\n' %str(authors.shape))\n",
    "# print(authors.head(), '\\n\\n')\n",
    "\n",
    "# read in paper authors map\n",
    "paper_authors = pd.read_csv('data/paper_authors.csv')\n",
    "paper_authors.set_index('id', inplace=True, drop=True)\n",
    "\n",
    "# print details about paper authors map\n",
    "print('Paper Author table:\\n', '*'*50)\n",
    "print('There exist %i papers authored by %i unique authors' \n",
    "     %(paper_authors.paper_id.nunique(), paper_authors.author_id.nunique()))\n",
    "paper_authors['paper_author_id'] = paper_authors['paper_id'].astype(str) + '_' + paper_authors['author_id'].astype(str)\n",
    "print('There are %i unique author/paper pairs' %paper_authors['paper_author_id'].nunique())\n",
    "print('Shape of Paper Author table %s\\n' %str(paper_authors.shape))\n",
    "# print(paper_authors.head(), '\\n\\n')\n",
    "\n",
    "# read in papers\n",
    "papers = pd.read_csv('data/papers.csv')\n",
    "papers.set_index('id', inplace=True)\n",
    "papers.index.name = 'paper_id'\n",
    "\n",
    "# print details about papers table\n",
    "print('Papers table:\\n', '*'*50)\n",
    "print('There exist %i papers with %i unique titles' \n",
    "     %(papers.index.nunique(), papers.title.nunique()))\n",
    "print('Shape of Papers table %s\\n' %str(papers.shape))\n",
    "# print(papers.head(), '\\n\\n')\n",
    "\n",
    "# map author names \n",
    "print('Combining data to one structure\\n', '*'*50)\n",
    "print('Mapping author names to the Paper Authors table in preparation to merge')\n",
    "author_map = dict(zip(authors.index.values, authors.name.values))\n",
    "paper_authors['author'] = paper_authors['author_id'].map(author_map)\n",
    "\n",
    "# outer join of papers/authors\n",
    "papers.reset_index(inplace=True, drop=False)\n",
    "df = paper_authors.merge(papers, on='paper_id', how='outer')\n",
    "\n",
    "# print outputs\n",
    "print('After merging there are %i missing values in the author field' %df.author.isnull().sum())\n",
    "df.loc[df.author.isnull(), 'author'] = 'UNKNOWN'\n",
    "print('Final shape of data %s\\n\\n' %str(df.shape))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>paper_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Oral</th>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poster</th>\n",
       "      <td>7253</td>\n",
       "      <td>7250</td>\n",
       "      <td>7250</td>\n",
       "      <td>7253</td>\n",
       "      <td>7253</td>\n",
       "      <td>7253</td>\n",
       "      <td>7253</td>\n",
       "      <td>7253</td>\n",
       "      <td>7253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spotlight</th>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            paper_id  author_id  paper_author_id  author  year  title  \\\n",
       "event_type                                                              \n",
       "Oral             351        351              351     351   351    351   \n",
       "Poster          7253       7250             7250    7253  7253   7253   \n",
       "Spotlight        550        550              550     550   550    550   \n",
       "\n",
       "            pdf_name  abstract  paper_text  \n",
       "event_type                                  \n",
       "Oral             351       351         351  \n",
       "Poster          7253      7253        7253  \n",
       "Spotlight        550       550         550  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('event_type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Michael I. Jordan     0.004846\n",
       "Bernhard Sch?lkopf    0.002975\n",
       "Yoshua Bengio         0.002879\n",
       "Geoffrey E. Hinton    0.002783\n",
       "Zoubin Ghahramani     0.002447\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# pareto principle for author output\n",
    "_ = df.groupby('author').title.count().sort_values(ascending=False) / df.groupby('author').title.count().sum()\n",
    "_.plot(figsize=(17, 9))\n",
    "_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# FILE: custom_classes.py\n",
    "# DESCR: text cleaner\n",
    "################################################################################\n",
    "\n",
    "# IMPORT STATEMENTS\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "import spacy\n",
    "from tld import get_tld\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "# CLASS DEFINITIONS\n",
    "class TextCleaner(TransformerMixin):\n",
    "    \"\"\"Text cleaning to slot into sklearn interface\"\"\"\n",
    "\n",
    "    def __init__(self, remove_stopwords=True, remove_urls=True,\n",
    "                 remove_puncts=True, lemmatize=True, extra_punct='',\n",
    "                 custom_stopwords=[], custom_non_stopwords = [],\n",
    "                 verbose=True, parser='big'):\n",
    "        \"\"\"\n",
    "        DESCR:\n",
    "        INPUT: remove_stopwords - bool - remove is, there, he etc...\n",
    "               remove_urls - bool - 't www.monkey.com t' --> 't com t'\n",
    "               remove_punct - bool - all punct and digits gone\n",
    "               lemmatize - bool - whether to apply lemmtization\n",
    "               extra_punct - str - other characters to remove\n",
    "               custom_stopwords - list - add to standard stops\n",
    "               custom_non_stopwords - list - make sure are kept\n",
    "               verbose - bool - whether to print progress statements\n",
    "               parser - str - 'big' or small, one keeps more, and is slower\n",
    "        OUTPUT: self - **due to other method, not this one\n",
    "        \"\"\"\n",
    "        # Initialize passed Attributes to specify operations\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_puncts = remove_puncts\n",
    "        self.lemmatize = lemmatize\n",
    "\n",
    "        # Change how operations work\n",
    "        self.custom_stopwords = custom_stopwords\n",
    "        self.custom_non_stopwords = custom_non_stopwords\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Set up punctation tranlation table\n",
    "        self.removals = string.punctuation + string.digits + extra_punct\n",
    "        self.trans_table = str.maketrans({key: None for key in self.removals})\n",
    "\n",
    "        #Load nlp model for parsing usage later\n",
    "        self.parser = spacy.load('en_core_web_sm', \n",
    "                                 disable=['parser','ner','textcat'])\n",
    "        #from spacy.lang.en import English\n",
    "        if parser == 'small':\n",
    "            self.parser = spacy.load('en')#English()\n",
    "\n",
    "        #Add custom stop words to nlp\n",
    "        for word in self.custom_stopwords:\n",
    "            self.parser.vocab[word].is_stop = True\n",
    "\n",
    "        #Set custom nlp words to be kept\n",
    "        for word in self.custom_non_stopwords:\n",
    "            self.parser.vocab[word].is_stop = False\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"take array of docs to clean array of docs\"\"\"\n",
    "        # Potential replace urls with tld ie www.monkey.com to com\n",
    "        if self.remove_urls:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"CHANGING URLS to TLDS...  \", end='')\n",
    "            X = [self.remove_url(doc) for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Potentially remove punctuation\n",
    "        if self.remove_puncts:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"REMOVING PUNCTUATION AND DIGITS... \", end='')\n",
    "            X = [doc.lower().translate(self.trans_table) for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Using Spacy to parse text\n",
    "        start_time = time()\n",
    "        if self.verbose:\n",
    "            print(\"PARSING TEXT WITH SPACY... \", end='')\n",
    "        #X = list(self.nlp.pipe(X))\n",
    "        X = list(self.parser.pipe(X))\n",
    "        if self.verbose:\n",
    "            print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Potential stopword removal\n",
    "        if self.remove_stopwords:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"REMOVING STOP WORDS FROM DOCUMENTS... \", end='')\n",
    "            X = [[word for word in doc if not word.is_stop] for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "\n",
    "        # Potential Lemmatization\n",
    "        if self.lemmatize:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"LEMMATIZING WORDS... \", end='')\n",
    "            X = [[word.lemma_ for word in doc] for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Put back to normal if no lemmatizing happened\n",
    "        if not self.lemmatize:\n",
    "            X = [[str(word).lower() for word in doc] for doc in X]\n",
    "\n",
    "        # Join Back up\n",
    "        return [' '.join(lst) for lst in X]\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"interface conforming, and allows use of fit_transform\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(text):\n",
    "        \"\"\"\n",
    "        DESCR: given a url string find urls and replace with top level domain\n",
    "               a bit lazy in that if there are multiple all are replaced by first\n",
    "        INPUT: text - str - 'this is www.monky.com in text'\n",
    "        OUTPIT: str - 'this is <com> in text'\n",
    "        \"\"\"\n",
    "        # Define string to match urls\n",
    "        url_re = '((?:www|https?)(://)?[^\\s]+)'\n",
    "\n",
    "        # Find potential things to replace\n",
    "        matches = re.findall(url_re, text)\n",
    "        if matches == []:\n",
    "            return text\n",
    "\n",
    "        # Get tld of first match\n",
    "        match = matches[0][0]\n",
    "        try:\n",
    "            tld = get_tld(match, fail_silently=True, fix_protocol=True)\n",
    "        except ValueError:\n",
    "            tld = None\n",
    "\n",
    "        # failures return none so change to empty\n",
    "        if tld is None:\n",
    "            tld = \"\"\n",
    "\n",
    "        # make this obvsiouyly an odd tag\n",
    "        tld = f\"<{tld}>\"\n",
    "\n",
    "        # Make replacements and return\n",
    "        return re.sub(url_re, tld, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text\n",
    "\n",
    "## Clean Titles of Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGING URLS to TLDS...  0 seconds\n",
      "REMOVING PUNCTUATION AND DIGITS... 0 seconds\n",
      "PARSING TEXT WITH SPACY... 7 seconds\n",
      "REMOVING STOP WORDS FROM DOCUMENTS... 0 seconds\n",
      "LEMMATIZING WORDS... 0 seconds\n"
     ]
    }
   ],
   "source": [
    "df['cleaned_title'] = TextCleaner().transform(df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Connectivity Versus Entropy</td>\n",
       "      <td>connectivity versus entropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stochastic Learning Networks and their Electro...</td>\n",
       "      <td>stochastic learning network electronic impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stochastic Learning Networks and their Electro...</td>\n",
       "      <td>stochastic learning network electronic impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stochastic Learning Networks and their Electro...</td>\n",
       "      <td>stochastic learning network electronic impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stochastic Learning Networks and their Electro...</td>\n",
       "      <td>stochastic learning network electronic impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Learning on a General Network</td>\n",
       "      <td>learn general network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An Artificial Neural Network for Spatio-Tempor...</td>\n",
       "      <td>artificial neural network spatiotemporal bipol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>An Artificial Neural Network for Spatio-Tempor...</td>\n",
       "      <td>artificial neural network spatiotemporal bipol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>An Artificial Neural Network for Spatio-Tempor...</td>\n",
       "      <td>artificial neural network spatiotemporal bipol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>On Properties of Networks of Neuron-Like Elements</td>\n",
       "      <td>property network neuronlike element</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>On Properties of Networks of Neuron-Like Elements</td>\n",
       "      <td>property network neuronlike element</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Supervised Learning of Probability Distributio...</td>\n",
       "      <td>supervised learning probability distribution n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Supervised Learning of Probability Distributio...</td>\n",
       "      <td>supervised learning probability distribution n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Centric Models of the Orientation Map in Prima...</td>\n",
       "      <td>centric model orientation map primary visual c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Centric Models of the Orientation Map in Prima...</td>\n",
       "      <td>centric model orientation map primary visual c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Analysis and Comparison of Different Learning ...</td>\n",
       "      <td>analysis comparison different learn algorithm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Simulations Suggest Information Processing Rol...</td>\n",
       "      <td>simulation suggest information processing role...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Optimal Neural Spike Classification</td>\n",
       "      <td>optimal neural spike classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Optimal Neural Spike Classification</td>\n",
       "      <td>optimal neural spike classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Neural Networks for Template Matching: Applica...</td>\n",
       "      <td>neural network template matching application r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                         Connectivity Versus Entropy   \n",
       "1   Stochastic Learning Networks and their Electro...   \n",
       "2   Stochastic Learning Networks and their Electro...   \n",
       "3   Stochastic Learning Networks and their Electro...   \n",
       "4   Stochastic Learning Networks and their Electro...   \n",
       "5                       Learning on a General Network   \n",
       "6   An Artificial Neural Network for Spatio-Tempor...   \n",
       "7   An Artificial Neural Network for Spatio-Tempor...   \n",
       "8   An Artificial Neural Network for Spatio-Tempor...   \n",
       "9   On Properties of Networks of Neuron-Like Elements   \n",
       "10  On Properties of Networks of Neuron-Like Elements   \n",
       "11  Supervised Learning of Probability Distributio...   \n",
       "12  Supervised Learning of Probability Distributio...   \n",
       "13  Centric Models of the Orientation Map in Prima...   \n",
       "14  Centric Models of the Orientation Map in Prima...   \n",
       "15  Analysis and Comparison of Different Learning ...   \n",
       "16  Simulations Suggest Information Processing Rol...   \n",
       "17                Optimal Neural Spike Classification   \n",
       "18                Optimal Neural Spike Classification   \n",
       "19  Neural Networks for Template Matching: Applica...   \n",
       "\n",
       "                                        cleaned_title  \n",
       "0                         connectivity versus entropy  \n",
       "1   stochastic learning network electronic impleme...  \n",
       "2   stochastic learning network electronic impleme...  \n",
       "3   stochastic learning network electronic impleme...  \n",
       "4   stochastic learning network electronic impleme...  \n",
       "5                               learn general network  \n",
       "6   artificial neural network spatiotemporal bipol...  \n",
       "7   artificial neural network spatiotemporal bipol...  \n",
       "8   artificial neural network spatiotemporal bipol...  \n",
       "9                 property network neuronlike element  \n",
       "10                property network neuronlike element  \n",
       "11  supervised learning probability distribution n...  \n",
       "12  supervised learning probability distribution n...  \n",
       "13  centric model orientation map primary visual c...  \n",
       "14  centric model orientation map primary visual c...  \n",
       "15  analysis comparison different learn algorithm ...  \n",
       "16  simulation suggest information processing role...  \n",
       "17                optimal neural spike classification  \n",
       "18                optimal neural spike classification  \n",
       "19  neural network template matching application r...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title', 'cleaned_title']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text of Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGING URLS to TLDS...  12 seconds\n",
      "REMOVING PUNCTUATION AND DIGITS... 1 seconds\n",
      "PARSING TEXT WITH SPACY... "
     ]
    }
   ],
   "source": [
    "df['cleaned_text'] = TextCleaner().transform(df.paper_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['paper_text', 'cleaned_text']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Text & Title, Weighting Title More Heavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['combined_cleaned_text'] = \n",
    "(df.cleaned_title.astype(str) + ' ')*5 #+ df.cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test `LatentDirichletAllocation` with `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from time import time\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    '''from sklearn example website for LDA'''\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "def count_vectorizer_lda(data_samples, n_features, n_components, n_top_words):\n",
    "    # Use tf (raw term count) features for LDA.\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                    max_features=n_features,\n",
    "                                    stop_words='english')\n",
    "    t0 = time()\n",
    "    tf = tf_vectorizer.fit_transform(data_samples)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    t0 = time()\n",
    "    lda.fit(tf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in LDA model:\")\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print_top_words(lda, tf_feature_names, n_top_words)\n",
    "    \n",
    "    return lda\n",
    "    \n",
    "\n",
    "n_features = 500\n",
    "n_components = 10\n",
    "n_top_words = 25\n",
    "\n",
    "count_vectorizer_lda(df['cleaned_text'], n_features, n_components, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out ToPMINE\n",
    "# check frequent item sets using Apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextNormalizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-c9f1cca8c9eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSklearnTopicModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-c9f1cca8c9eb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_topics, estimator)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         self.model = Pipeline([\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             ('tfidf', CountVectorizer(tokenizer=identity,\n\u001b[1;32m     35\u001b[0m                                       preprocessor=None, lowercase=False)),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextNormalizer' is not defined"
     ]
    }
   ],
   "source": [
    "#from reader import PickledCorpusReader\n",
    "#from transformers import TextNormalizer, GensimTfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.sklearn_api import lsimodel, ldamodel\n",
    "\n",
    "def identity(words):\n",
    "    return words\n",
    "\n",
    "\n",
    "class SklearnTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA',\n",
    "        To use Non-Negative Matrix Factorization, set estimator to 'NMF',\n",
    "        otherwise, defaults to Latent Dirichlet Allocation ('LDA').\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
    "        elif estimator == 'NMF':\n",
    "            self.estimator = NMF(n_components=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = LatentDirichletAllocation(n_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('tfidf', CountVectorizer(tokenizer=identity,\n",
    "                                      preprocessor=None, lowercase=False)),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.model.fit_transform(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def get_topics(self, n=25):\n",
    "        \"\"\"\n",
    "        n is the number of top terms to show for each topic\n",
    "        \"\"\"\n",
    "        vectorizer = self.model.named_steps['tfidf']\n",
    "        model = self.model.steps[-1][1]\n",
    "        names = vectorizer.get_feature_names()\n",
    "        topics = dict()\n",
    "\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            features = topic.argsort()[:-(n - 1): -1]\n",
    "            tokens = [names[i] for i in features]\n",
    "            topics[idx] = tokens\n",
    "\n",
    "        return topics\n",
    "\n",
    "\n",
    "df['processed_title'] = SklearnTopicModels().fit_transform(df.title)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(norm=None) #Ignoring the norm (ie. no Euclidean norm)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.get_shape())\n",
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PickledCorpusReader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-aee1bdeffb33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPickledCorpusReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPickledCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../corpus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PickledCorpusReader'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.sklearn_api import lsimodel, ldamodel\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [\n",
    "            self.normalize(document)\n",
    "            for document in documents\n",
    "        ]\n",
    "\n",
    "\n",
    "class GensimTfidfVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dirpath=\".\", tofull=False):\n",
    "        \"\"\"\n",
    "        Pass in a directory that holds the lexicon in corpus.dict and the\n",
    "        TFIDF model in tfidf.model (for now).\n",
    "\n",
    "        Set tofull = True if the next thing is a Scikit-Learn estimator\n",
    "        otherwise keep False if the next thing is a Gensim model.\n",
    "        \"\"\"\n",
    "        self._lexicon_path = os.path.join(dirpath, \"corpus.dict\")\n",
    "        self._tfidf_path = os.path.join(dirpath, \"tfidf.model\")\n",
    "\n",
    "        self.lexicon = None\n",
    "        self.tfidf = None\n",
    "        self.tofull = tofull\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        if os.path.exists(self._lexicon_path):\n",
    "            self.lexicon = Dictionary.load(self._lexicon_path)\n",
    "\n",
    "        if os.path.exists(self._tfidf_path):\n",
    "            self.tfidf = TfidfModel().load(self._tfidf_path)\n",
    "\n",
    "    def save(self):\n",
    "        self.lexicon.save(self._lexicon_path)\n",
    "        self.tfidf.save(self._tfidf_path)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        self.lexicon = Dictionary(documents)\n",
    "        self.tfidf = TfidfModel([self.lexicon.doc2bow(doc) for doc in documents], id2word=self.lexicon)\n",
    "        self.save()\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        def generator():\n",
    "            for document in documents:\n",
    "                vec = self.tfidf[self.lexicon.doc2bow(document)]\n",
    "                if self.tofull:\n",
    "                    yield sparse2full(vec)\n",
    "                else:\n",
    "                    yield vec\n",
    "        return list(generator())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from reader import PickledCorpusReader\n",
    "\n",
    "    corpus = PickledCorpusReader('../corpus')\n",
    "    docs = [\n",
    "        list(corpus.docs(fileids=fileid))[0]\n",
    "        for fileid in corpus.fileids()\n",
    "    ]\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('vect', GensimTfidfVectorizer()),\n",
    "        ('lda', ldamodel.LdaTransformer())])\n",
    "\n",
    "    model.fit_transform(docs)\n",
    "\n",
    "    print(model.named_steps['norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-0f0db8d87c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPickledCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../corpus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# With Sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "class GensimTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA'\n",
    "        otherwise defaults to Latent Dirichlet Allocation.\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = lsimodel.LsiTransformer(num_topics=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = ldamodel.LdaTransformer(num_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('vect', GensimTfidfVectorizer()),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "\n",
    "    def fit(self, documents):\n",
    "        self.model.fit(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = PickledCorpusReader('../corpus')\n",
    "\n",
    "    # With Sklearn\n",
    "    skmodel = SklearnTopicModels(estimator='NMF')\n",
    "    documents   = corpus.docs()\n",
    "\n",
    "    skmodel.fit_transform(documents)\n",
    "    topics = skmodel.get_topics()\n",
    "    for topic, terms in topics.items():\n",
    "        print(\"Topic #{}:\".format(topic+1))\n",
    "        print(terms)\n",
    "\n",
    "    # # With Gensim\n",
    "    # gmodel = GensimTopicModels(estimator='LSA')\n",
    "    #\n",
    "    # docs = [\n",
    "    #     list(corpus.docs(fileids=fileid))[0]\n",
    "    #     for fileid in corpus.fileids()\n",
    "    # ]\n",
    "    #\n",
    "    # gmodel.fit(docs)\n",
    "    #\n",
    "    # # retrieve the fitted lsa model from the named steps of the pipeline\n",
    "    # lsa = gmodel.model.named_steps['lsa'].gensim_model\n",
    "    #\n",
    "    # # show the topics with the token-weights for the top 10 most influential tokens:\n",
    "    # print(lsa.print_topics(10))\n",
    "\n",
    "\n",
    "    # # retrieve the fitted lda model from the named steps of the pipeline\n",
    "    # lda = gmodel.model.named_steps['lda'].gensim_model\n",
    "    #\n",
    "    # # show the topics with the token-weights for the top 10 most influential tokens:\n",
    "    # lda.print_topics(10)\n",
    "\n",
    "    # corpus = [\n",
    "    #     gmodel.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
    "    #     for doc in gmodel.model.named_steps['norm'].transform(docs)\n",
    "    # ]\n",
    "    #\n",
    "    #\n",
    "    # id2token = gmodel.model.named_steps['vect'].lexicon.id2token\n",
    "    #\n",
    "    # for word_id, freq in next(iter(corpus)):\n",
    "    #     print(id2token[word_id], freq)\n",
    "\n",
    "    # # get the highest weighted topic for each of the documents in the corpus\n",
    "    # def get_topics(vectorized_corpus, model):\n",
    "    #     from operator import itemgetter\n",
    "    #\n",
    "    #     topics = [\n",
    "    #         max(model[doc], key=itemgetter(1))[0]\n",
    "    #         for doc in vectorized_corpus\n",
    "    #     ]\n",
    "    #\n",
    "    #     return topics\n",
    "    #\n",
    "    # topics = get_topics(corpus,lda)\n",
    "    #\n",
    "    # for topic, doc in zip(topics, docs):\n",
    "    #     print(\"Topic:{}\".format(topic))\n",
    "    #     print(doc)\n",
    "    #\n",
    "    ## retreive the fitted vectorizer or the lexicon if needed\n",
    "    # tfidf = gmodel.model.named_steps['vect'].tfidf\n",
    "    # lexicon = gmodel.model.named_steps['vect'].lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

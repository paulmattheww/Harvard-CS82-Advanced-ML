{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "import spacy\n",
    "from tld import get_tld\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hisashi Suzuki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>David Brady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Santosh S. Venkatesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Charles Fefferman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Artur Speiser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name\n",
       "id                         \n",
       "1            Hisashi Suzuki\n",
       "10              David Brady\n",
       "100    Santosh S. Venkatesh\n",
       "1000      Charles Fefferman\n",
       "10000         Artur Speiser"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('data/authors.csv')\n",
    "authors.set_index('id', drop=True, inplace=True)\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9784, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  paper_id  author_id\n",
       "0   1        63         94\n",
       "1   2        80        124\n",
       "2   3        80        125\n",
       "3   4        80        126\n",
       "4   5        80        127"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_authors = pd.read_csv('data/paper_authors.csv')\n",
    "paper_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('data/papers.csv')\n",
    "papers.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract',\n",
       "       'paper_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextCleaner(TransformerMixin):\n",
    "    \"\"\"Text cleaning to slot into sklearn interface\"\"\"\n",
    "\n",
    "    def __init__(self, remove_stopwords=True, remove_urls=True,\n",
    "                 remove_puncts=True, lemmatize=True, extra_punct='',\n",
    "                 custom_stopwords=[], custom_non_stopwords = [],\n",
    "                 verbose=True, parser='big', addl_stop_words=None):\n",
    "        \"\"\"\n",
    "        DESCR:\n",
    "        INPUT: remove_stopwords - bool - remove is, there, he etc...\n",
    "               remove_urls - bool - 't www.monkey.com t' --> 't com t'\n",
    "               remove_punct - bool - all punct and digits gone\n",
    "               lemmatize - bool - whether to apply lemmtization\n",
    "               extra_punct - str - other characters to remove\n",
    "               custom_stopwords - list - add to standard stops\n",
    "               custom_non_stopwords - list - make sure are kept\n",
    "               verbose - bool - whether to print progress statements\n",
    "               parser - str - 'big' or small, one keeps more, and is slower\n",
    "        OUTPUT: self - **due to other method, not this one\n",
    "        \"\"\"\n",
    "        # Initialize passed Attributes to specify operations\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_puncts = remove_puncts\n",
    "        self.lemmatize = lemmatize\n",
    "\n",
    "        # Change how operations work\n",
    "        self.custom_stopwords = custom_stopwords\n",
    "        self.custom_non_stopwords = custom_non_stopwords\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Set up punctation tranlation table\n",
    "        self.removals = string.punctuation + string.digits + extra_punct\n",
    "        self.trans_table = str.maketrans({key: None for key in self.removals})\n",
    "\n",
    "        # Load nlp model for parsing usage later\n",
    "        self.parser = spacy.load('en_core_web_sm', \n",
    "                                 disable=['parser','ner','textcat'])\n",
    "        # from spacy.lang.en import English\n",
    "        if parser == 'small':\n",
    "            self.parser = spacy.load('en')#English()\n",
    "\n",
    "        # Add custom stop words to nlp\n",
    "        for word in self.custom_stopwords:\n",
    "            self.parser.vocab[word].is_stop = True\n",
    "\n",
    "        # Set custom nlp words to be kept\n",
    "        for word in self.custom_non_stopwords:\n",
    "            self.parser.vocab[word].is_stop = False\n",
    "        \n",
    "        # account for additional stop words\n",
    "        if addl_stop_words is not None:\n",
    "            for word in self.addl_stop_words:\n",
    "                self.parser.vocab[word] = True\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"take array of docs to clean array of docs\"\"\"\n",
    "        # Potential replace urls with tld ie www.monkey.com to com\n",
    "        if self.remove_urls:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"CHANGING URLS to TLDS...  \", end='')\n",
    "            X = [self.remove_url(doc) for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Potentially remove punctuation\n",
    "        if self.remove_puncts:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"REMOVING PUNCTUATION AND DIGITS... \", end='')\n",
    "            X = [doc.lower().translate(self.trans_table) for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Using Spacy to parse text\n",
    "        start_time = time()\n",
    "        if self.verbose:\n",
    "            print(\"PARSING TEXT WITH SPACY... \", end='')\n",
    "        X = list(self.parser.pipe(X))\n",
    "        if self.verbose:\n",
    "            print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Potential stopword removal\n",
    "        if self.remove_stopwords:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"REMOVING STOP WORDS FROM DOCUMENTS... \", end='')\n",
    "            X = [[word for word in doc if not word.is_stop] for doc in X]\n",
    "            if addl_stop_words is not None:\n",
    "                # remove additional stop words\n",
    "                X = [[word for word in doc if word not in addl_stop_words] for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "\n",
    "        # Potential Lemmatization\n",
    "        if self.lemmatize:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"LEMMATIZING WORDS... \", end='')\n",
    "            X = [[word.lemma_ for word in doc] for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Put back to normal if no lemmatizing happened\n",
    "        if not self.lemmatize:\n",
    "            X = [[str(word).lower() for word in doc] for doc in X]\n",
    "\n",
    "        # Join Back up\n",
    "        return [' '.join(lst) for lst in X]\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"interface conforming, and allows use of fit_transform\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(text):\n",
    "        \"\"\"\n",
    "        DESCR: given a url string find urls and replace with top level domain\n",
    "               a bit lazy in that if there are multiple all are replaced by first\n",
    "        INPUT: text - str - 'this is www.monky.com in text'\n",
    "        OUTPIT: str - 'this is <com> in text'\n",
    "        \"\"\"\n",
    "        # Define string to match urls\n",
    "        url_re = '((?:www|https?)(://)?[^\\s]+)'\n",
    "\n",
    "        # Find potential things to replace\n",
    "        matches = re.findall(url_re, text)\n",
    "        if matches == []:\n",
    "            return text\n",
    "\n",
    "        # Get tld of first match\n",
    "        match = matches[0][0]\n",
    "        try:\n",
    "            tld = get_tld(match, fail_silently=True, fix_protocol=True)\n",
    "        except ValueError:\n",
    "            tld = None\n",
    "\n",
    "        # failures return none so change to empty\n",
    "        if tld is None:\n",
    "            tld = \"\"\n",
    "\n",
    "        # make this obvsiouyly an odd tag\n",
    "        tld = f\"<{tld}>\"\n",
    "\n",
    "        # Make replacements and return\n",
    "        return re.sub(url_re, tld, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

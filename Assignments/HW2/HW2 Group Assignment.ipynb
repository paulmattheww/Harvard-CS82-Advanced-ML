{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sept 16, 2018 Trend Detection CSCI E-82 Homework 2\n",
    "### Due: October 1, 2018 11:59pm EST\n",
    "\n",
    "## Overview\n",
    "\n",
    "***Identifying technology trends is of core importance to venture capitalists, companies and individuals who may invest money or time to pursue the hottest areas. Using historic data, the goals are to characterize either an increase or decrease in certain areas over a span of time, and use that information to predict the next areas before everyone becomes aware of the trend. Economists and financial traders routinely develop methods to achieve this goal using numeric data, but that’s a different problem.***\n",
    "\n",
    "Mining published literature for trend detection is not a new area, but it is far from being adequately solved. There are a number of papers that describe case studies for a given area, but none offer a definitive approach; most focus on only a niche area. The two main approaches to the problem are using word concepts and citation networks. The word concept approach aims to characterize a subfield by its component terms automatically and then look for patterns over time. Google Trends offers a plot of word frequency over time, but subfields tend to be more complex in that “convolutional neural network” has synonyms or abbreviations (CNN) that can be ambiguous. Furthermore, as areas mature, the concepts may refine into distinct groups and associate with specific sets of terms. The citation network looks for patterns in which authors are referenced to characterize concepts. These can be used to separate different areas based on which paper is cited, but also tend to be fairly noisy.\n",
    "This homework will give you and a required partner a chance to develop your text mining skills to computationally find the top 10 upward or downward trending areas within the context of 30 years of the Neural Information Processing Systems (NIPS) proceedings for their annual conference.\n",
    "\n",
    "## Data Set\n",
    "The official data set is the NIPS Proceedings available at https://papers.nips.cc/. However, this will take a long time to download and hammer their server so we will would like to provide you with alternatives. There is a version of the dataset here: https://www.kaggle.com/benhamner/nips-papers. You will need a Kaggle login in order to download it. Since I would prefer everyone spend more time on the analysis and less time on the cleaning, I am working to put out a slightly cleaner version of the official data set shortly that I will post.\n",
    "Partners:\n",
    "  HW2 is a partnered homework so work should be completed with 1 partner. To help everyone find a partner, we ask you to sign up by putting your partner's first name next to yours and vice versa using this shared spreadsheet: https://docs.google.com/spreadsheets/d/1oz0pNYx8X2WptwiLsD9zMUtsCVZiEUTFXZ5DEnPaewk/edit?u\n",
    " sp=sharing. This will give everyone immediate feedback on who doesn't have a partner.\n",
    "To select a partner, the self-intros on piazza are a good place to start. Please use the Canvas email to contact them since we respect your privacy and don't want to post everyone's email.\n",
    "  \n",
    "## Suggestions on Strategies\n",
    "You are welcome to pursue any approach. If you find applicable methods online, feel free to use them and be sure to cite the results. I would recommend starting with the text mining pipeline described in lecture and section to clean the documents and identify single- and perhaps multi-word terms. In this case, the first pass might be to perform simple counting as a baseline over time and work for a standard approach to plot trends taking the normalization into account. In the next pass, you might expand from the isolated word terms to synonyms to larger concept subfields that may cluster together. The citations or co-related words can be helpful for this. Further refinements might be to include only certain sections of the documents or try weighting schemes.\n",
    "\n",
    "## Grading Philosophy\n",
    "We will grade based on 1) your success in the project so label your final result, and 2) your exploration of different ideas. Please document your success, but also document your rationale and failed approaches. We want to know which hypotheses you pursued and how they panned out. With these kinds of homework, we expect both partners to work together and contribute equally to a greater result than either could do alone given the time constraints. We will post a form to assess your partner’s contribution relative to yours.\n",
    "\n",
    "## What to Submit\n",
    "Please submit your python notebook and associated pdf of that notebook. In a separate document, please also submit a brief description (1-2 paragraphs each) as a separate document to address the following:\n",
    "1. How have you defined a trend? How can you separate it from background noise and/or spurious relationships?\n",
    "2. What are the main techniques you have used and how have you tailored them for this problem?\n",
    "3. What was your strategy for finding multi-word phrases versus single words?\n",
    "4. What approach(es) did you use to separate one subfield from others?\n",
    "5. What parts of the document did you use and why?\n",
    "6. How did you normalize the results against the growth of the conference, lengths of documents, etc.?\n",
    "7. We know that you can look back and find trends but how would you find the next trend with your method? Be specific.\n",
    "8. Plot of the final top 10 normalized trends as a function of time.\n",
    "\n",
    "To assist the grading within the notebook:\n",
    "\n",
    "* Label your final approach within the file for grading purposes.\n",
    "* Flag the distinct approaches with a header describing your strategy and corresponding results.\n",
    "\n",
    "It makes it much easier to follow your rationale with headers and descriptions than trying guess using the code alone.\n",
    "We hope that you find this to be an interesting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "import spacy\n",
    "from tld import get_tld\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hisashi Suzuki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>David Brady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Santosh S. Venkatesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Charles Fefferman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Artur Speiser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name\n",
       "id                         \n",
       "1            Hisashi Suzuki\n",
       "10              David Brady\n",
       "100    Santosh S. Venkatesh\n",
       "1000      Charles Fefferman\n",
       "10000         Artur Speiser"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('data/authors.csv')\n",
    "authors.set_index('id', drop=True, inplace=True)\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9784, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  paper_id  author_id\n",
       "0   1        63         94\n",
       "1   2        80        124\n",
       "2   3        80        125\n",
       "3   4        80        126\n",
       "4   5        80        127"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_authors = pd.read_csv('data/paper_authors.csv')\n",
    "paper_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('data/papers.csv')\n",
    "papers.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year', 'title', 'event_type', 'pdf_name', 'abstract',\n",
       "       'paper_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextCleaner(TransformerMixin):\n",
    "    \"\"\"Text cleaning to slot into sklearn interface\"\"\"\n",
    "\n",
    "    def __init__(self, remove_stopwords=True, remove_urls=True,\n",
    "                 remove_puncts=True, lemmatize=True, extra_punct='',\n",
    "                 custom_stopwords=[], custom_non_stopwords = [],\n",
    "                 verbose=True, parser='big'):\n",
    "        \"\"\"\n",
    "        DESCR:\n",
    "        INPUT: remove_stopwords - bool - remove is, there, he etc...\n",
    "               remove_urls - bool - 't www.monkey.com t' --> 't com t'\n",
    "               remove_punct - bool - all punct and digits gone\n",
    "               lemmatize - bool - whether to apply lemmtization\n",
    "               extra_punct - str - other characters to remove\n",
    "               custom_stopwords - list - add to standard stops\n",
    "               custom_non_stopwords - list - make sure are kept\n",
    "               verbose - bool - whether to print progress statements\n",
    "               parser - str - 'big' or small, one keeps more, and is slower\n",
    "        OUTPUT: self - **due to other method, not this one\n",
    "        \"\"\"\n",
    "        # Initialize passed Attributes to specify operations\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_puncts = remove_puncts\n",
    "        self.lemmatize = lemmatize\n",
    "\n",
    "        # Change how operations work\n",
    "        self.custom_stopwords = custom_stopwords\n",
    "        self.custom_non_stopwords = custom_non_stopwords\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Set up punctation tranlation table\n",
    "        self.removals = string.punctuation + string.digits + extra_punct\n",
    "        self.trans_table = str.maketrans({key: None for key in self.removals})\n",
    "\n",
    "        # Load nlp model for parsing usage later\n",
    "        self.parser = spacy.load('en_core_web_sm', \n",
    "                                 disable=['parser','ner','textcat'])\n",
    "        # from spacy.lang.en import English\n",
    "        if parser == 'small':\n",
    "            self.parser = spacy.load('en')#English()\n",
    "\n",
    "        # Add custom stop words to nlp\n",
    "        for word in self.custom_stopwords:\n",
    "            self.parser.vocab[word].is_stop = True\n",
    "\n",
    "        # Set custom nlp words to be kept\n",
    "        for word in self.custom_non_stopwords:\n",
    "            self.parser.vocab[word].is_stop = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"take array of docs to clean array of docs\"\"\"\n",
    "        # Potential replace urls with tld ie www.monkey.com to com\n",
    "        if self.remove_urls:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"CHANGING URLS to TLDS...  \", end='')\n",
    "            X = [self.remove_url(doc) for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Potentially remove punctuation\n",
    "        if self.remove_puncts:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"REMOVING PUNCTUATION AND DIGITS... \", end='')\n",
    "            X = [doc.lower().translate(self.trans_table) for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Using Spacy to parse text\n",
    "        start_time = time()\n",
    "        if self.verbose:\n",
    "            print(\"PARSING TEXT WITH SPACY... \", end='')\n",
    "        X = list(self.parser.pipe(X))\n",
    "        if self.verbose:\n",
    "            print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Potential stopword removal\n",
    "        if self.remove_stopwords:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"REMOVING STOP WORDS FROM DOCUMENTS... \", end='')\n",
    "            X = [[word for word in doc if not word.is_stop] for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "\n",
    "        # Potential Lemmatization\n",
    "        if self.lemmatize:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"LEMMATIZING WORDS... \", end='')\n",
    "            X = [[word.lemma_ for word in doc] for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Put back to normal if no lemmatizing happened\n",
    "        if not self.lemmatize:\n",
    "            X = [[str(word).lower() for word in doc] for doc in X]\n",
    "\n",
    "        # Join Back up\n",
    "        return [' '.join(lst) for lst in X]\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"interface conforming, and allows use of fit_transform\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(text):\n",
    "        \"\"\"\n",
    "        DESCR: given a url string find urls and replace with top level domain\n",
    "               a bit lazy in that if there are multiple all are replaced by first\n",
    "        INPUT: text - str - 'this is www.monky.com in text'\n",
    "        OUTPIT: str - 'this is <com> in text'\n",
    "        \"\"\"\n",
    "        # Define string to match urls\n",
    "        url_re = '((?:www|https?)(://)?[^\\s]+)'\n",
    "\n",
    "        # Find potential things to replace\n",
    "        matches = re.findall(url_re, text)\n",
    "        if matches == []:\n",
    "            return text\n",
    "\n",
    "        # Get tld of first match\n",
    "        match = matches[0][0]\n",
    "        try:\n",
    "            tld = get_tld(match, fail_silently=True, fix_protocol=True)\n",
    "        except ValueError:\n",
    "            tld = None\n",
    "\n",
    "        # failures return none so change to empty\n",
    "        if tld is None:\n",
    "            tld = \"\"\n",
    "\n",
    "        # make this obvsiouyly an odd tag\n",
    "        tld = f\"<{tld}>\"\n",
    "\n",
    "        # Make replacements and return\n",
    "        return re.sub(url_re, tld, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

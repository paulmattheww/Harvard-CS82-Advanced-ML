{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sept 16, 2018 Trend Detection CSCI E-82 Homework 2\n",
    "### Due: October 1, 2018 11:59pm EST\n",
    "\n",
    "## Overview\n",
    "\n",
    "***Identifying technology trends is of core importance to venture capitalists, companies and individuals who may invest money or time to pursue the hottest areas. Using historic data, the goals are to characterize either an increase or decrease in certain areas over a span of time, and use that information to predict the next areas before everyone becomes aware of the trend. Economists and financial traders routinely develop methods to achieve this goal using numeric data, but that’s a different problem.***\n",
    "\n",
    "Mining published literature for trend detection is not a new area, but it is far from being adequately solved. There are a number of papers that describe case studies for a given area, but none offer a definitive approach; most focus on only a niche area. The two main approaches to the problem are using word concepts and citation networks. The word concept approach aims to characterize a subfield by its component terms automatically and then look for patterns over time. Google Trends offers a plot of word frequency over time, but subfields tend to be more complex in that “convolutional neural network” has synonyms or abbreviations (CNN) that can be ambiguous. Furthermore, as areas mature, the concepts may refine into distinct groups and associate with specific sets of terms. The citation network looks for patterns in which authors are referenced to characterize concepts. These can be used to separate different areas based on which paper is cited, but also tend to be fairly noisy.\n",
    "This homework will give you and a required partner a chance to develop your text mining skills to computationally find the top 10 upward or downward trending areas within the context of 30 years of the Neural Information Processing Systems (NIPS) proceedings for their annual conference.\n",
    "\n",
    "## Data Set\n",
    "The official data set is the NIPS Proceedings available at https://papers.nips.cc/. However, this will take a long time to download and hammer their server so we will would like to provide you with alternatives. There is a version of the dataset here: https://www.kaggle.com/benhamner/nips-papers. You will need a Kaggle login in order to download it. Since I would prefer everyone spend more time on the analysis and less time on the cleaning, I am working to put out a slightly cleaner version of the official data set shortly that I will post.\n",
    "Partners:\n",
    "  HW2 is a partnered homework so work should be completed with 1 partner. To help everyone find a partner, we ask you to sign up by putting your partner's first name next to yours and vice versa using this shared spreadsheet: https://docs.google.com/spreadsheets/d/1oz0pNYx8X2WptwiLsD9zMUtsCVZiEUTFXZ5DEnPaewk/edit?u\n",
    " sp=sharing. This will give everyone immediate feedback on who doesn't have a partner.\n",
    "To select a partner, the self-intros on piazza are a good place to start. Please use the Canvas email to contact them since we respect your privacy and don't want to post everyone's email.\n",
    "  \n",
    "## Suggestions on Strategies\n",
    "You are welcome to pursue any approach. If you find applicable methods online, feel free to use them and be sure to cite the results. I would recommend starting with the text mining pipeline described in lecture and section to clean the documents and identify single- and perhaps multi-word terms. In this case, the first pass might be to perform simple counting as a baseline over time and work for a standard approach to plot trends taking the normalization into account. In the next pass, you might expand from the isolated word terms to synonyms to larger concept subfields that may cluster together. The citations or co-related words can be helpful for this. Further refinements might be to include only certain sections of the documents or try weighting schemes.\n",
    "\n",
    "## Grading Philosophy\n",
    "We will grade based on 1) your success in the project so label your final result, and 2) your exploration of different ideas. Please document your success, but also document your rationale and failed approaches. We want to know which hypotheses you pursued and how they panned out. With these kinds of homework, we expect both partners to work together and contribute equally to a greater result than either could do alone given the time constraints. We will post a form to assess your partner’s contribution relative to yours.\n",
    "\n",
    "## What to Submit\n",
    "Please submit your python notebook and associated pdf of that notebook. In a separate document, please also submit a brief description (1-2 paragraphs each) as a separate document to address the following:\n",
    "1. How have you defined a trend? How can you separate it from background noise and/or spurious relationships?\n",
    "2. What are the main techniques you have used and how have you tailored them for this problem?\n",
    "3. What was your strategy for finding multi-word phrases versus single words?\n",
    "4. What approach(es) did you use to separate one subfield from others?\n",
    "5. What parts of the document did you use and why?\n",
    "6. How did you normalize the results against the growth of the conference, lengths of documents, etc.?\n",
    "7. We know that you can look back and find trends but how would you find the next trend with your method? Be specific.\n",
    "8. Plot of the final top 10 normalized trends as a function of time.\n",
    "\n",
    "To assist the grading within the notebook:\n",
    "\n",
    "* Label your final approach within the file for grading purposes.\n",
    "* Flag the distinct approaches with a header describing your strategy and corresponding results.\n",
    "\n",
    "It makes it much easier to follow your rationale with headers and descriptions than trying guess using the code alone.\n",
    "We hope that you find this to be an interesting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "import spacy\n",
    "from tld import get_tld\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hisashi Suzuki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>David Brady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Santosh S. Venkatesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Charles Fefferman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Artur Speiser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name\n",
       "id                         \n",
       "1            Hisashi Suzuki\n",
       "10              David Brady\n",
       "100    Santosh S. Venkatesh\n",
       "1000      Charles Fefferman\n",
       "10000         Artur Speiser"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('data/authors.csv')\n",
    "authors.set_index('id', drop=True, inplace=True)\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    1,    10,   100,  1000, 10000, 10001, 10002, 10003, 10004,\n",
       "            10005,\n",
       "            ...\n",
       "             9990,  9991,  9992,  9993,  9994,  9995,  9996,  9997,  9998,\n",
       "             9999],\n",
       "           dtype='int64', name='id', length=9784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9784, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id  author_id\n",
       "id                     \n",
       "1         63         94\n",
       "2         80        124\n",
       "3         80        125\n",
       "4         80        126\n",
       "5         80        127"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_authors = pd.read_csv('data/paper_authors.csv')\n",
    "paper_authors.set_index('id', inplace=True, drop=True)\n",
    "paper_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   1,   10,  100, 1000, 1001, 1002, 1003, 1004, 1005, 1006,\n",
       "            ...\n",
       "              99,  990,  991,  992,  993,  994,  996,  997,  998,  999],\n",
       "           dtype='int64', name='id', length=7241)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                              title event_type  \\\n",
       "id                                                                         \n",
       "1     1987  Self-Organization of Associative Database and ...        NaN   \n",
       "10    1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "100   1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                               pdf_name          abstract  \\\n",
       "id                                                                          \n",
       "1     1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "10    10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "100   100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "1000  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "1001  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                             paper_text  \n",
       "id                                                       \n",
       "1     767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "10    683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "100   394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "1000  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "1001  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('data/papers.csv')\n",
    "papers.set_index('id', inplace=True)\n",
    "papers.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20838, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_authors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20838, 9)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.join(paper_authors, how='outer').join(authors, how='outer').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.join(paper_authors).join(authors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "      <td>63</td>\n",
       "      <td>94</td>\n",
       "      <td>Hisashi Suzuki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>The Capacity of the Kanerva Associative Memory...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-the-capacity-of-the-kanerva-associative-memo...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>184\\n\\nTHE CAPACITY OF THE KANERVA ASSOCIATIVE...</td>\n",
       "      <td>80</td>\n",
       "      <td>124</td>\n",
       "      <td>Suguru Arimoto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>Supervised Learning of Probability Distributio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3-supervised-learning-of-probability-distribut...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>52\\n\\nSupervised Learning of Probability Distr...</td>\n",
       "      <td>80</td>\n",
       "      <td>125</td>\n",
       "      <td>Philip A. Chou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>Constrained Differential Optimization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4-constrained-differential-optimization.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>612\\n\\nConstrained Differential Optimization\\n...</td>\n",
       "      <td>80</td>\n",
       "      <td>126</td>\n",
       "      <td>John C. Platt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>Towards an Organizing Principle for a Layered ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5-towards-an-organizing-principle-for-a-layere...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>485\\n\\nTOWARDS AN ORGANIZING PRINCIPLE FOR\\nA ...</td>\n",
       "      <td>80</td>\n",
       "      <td>127</td>\n",
       "      <td>Alan H. Barr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                              title event_type  \\\n",
       "id                                                                         \n",
       "1   1987.0  Self-Organization of Associative Database and ...        NaN   \n",
       "2   1987.0  The Capacity of the Kanerva Associative Memory...        NaN   \n",
       "3   1987.0  Supervised Learning of Probability Distributio...        NaN   \n",
       "4   1987.0              Constrained Differential Optimization        NaN   \n",
       "5   1987.0  Towards an Organizing Principle for a Layered ...        NaN   \n",
       "\n",
       "                                             pdf_name          abstract  \\\n",
       "id                                                                        \n",
       "1   1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "2   2-the-capacity-of-the-kanerva-associative-memo...  Abstract Missing   \n",
       "3   3-supervised-learning-of-probability-distribut...  Abstract Missing   \n",
       "4         4-constrained-differential-optimization.pdf  Abstract Missing   \n",
       "5   5-towards-an-organizing-principle-for-a-layere...  Abstract Missing   \n",
       "\n",
       "                                           paper_text  paper_id  author_id  \\\n",
       "id                                                                           \n",
       "1   767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...        63         94   \n",
       "2   184\\n\\nTHE CAPACITY OF THE KANERVA ASSOCIATIVE...        80        124   \n",
       "3   52\\n\\nSupervised Learning of Probability Distr...        80        125   \n",
       "4   612\\n\\nConstrained Differential Optimization\\n...        80        126   \n",
       "5   485\\n\\nTOWARDS AN ORGANIZING PRINCIPLE FOR\\nA ...        80        127   \n",
       "\n",
       "              name  \n",
       "id                  \n",
       "1   Hisashi Suzuki  \n",
       "2   Suguru Arimoto  \n",
       "3   Philip A. Chou  \n",
       "4    John C. Platt  \n",
       "5     Alan H. Barr  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = papers.join(paper_authors, how='outer').join(authors, how='outer')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Oral</th>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poster</th>\n",
       "      <td>2146</td>\n",
       "      <td>2146</td>\n",
       "      <td>2146</td>\n",
       "      <td>2146</td>\n",
       "      <td>2146</td>\n",
       "      <td>2146</td>\n",
       "      <td>2146</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spotlight</th>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            year  title  pdf_name  abstract  paper_text  paper_id  author_id  \\\n",
       "event_type                                                                     \n",
       "Oral          95     95        95        95          95        95         95   \n",
       "Poster      2146   2146      2146      2146        2146      2146       2146   \n",
       "Spotlight    181    181       181       181         181       181        181   \n",
       "\n",
       "            name  \n",
       "event_type        \n",
       "Oral          77  \n",
       "Poster      1616  \n",
       "Spotlight    115  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('event_type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xa2d76ed30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# diverse authors\n",
    "_ = df.groupby('name').title.count().sort_values(ascending=False) / df.groupby('name').title.count().sum()\n",
    "_.plot(figsize=(17, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# FILE: custom_classes.py\n",
    "# DESCR: text cleaner\n",
    "################################################################################\n",
    "\n",
    "# IMPORT STATEMENTS\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "import spacy\n",
    "from tld import get_tld\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "# CLASS DEFINITIONS\n",
    "class TextCleaner(TransformerMixin):\n",
    "    \"\"\"Text cleaning to slot into sklearn interface\"\"\"\n",
    "\n",
    "    def __init__(self, remove_stopwords=True, remove_urls=True,\n",
    "                 remove_puncts=True, lemmatize=True, extra_punct='',\n",
    "                 custom_stopwords=[], custom_non_stopwords = [],\n",
    "                 verbose=True, parser='big'):\n",
    "        \"\"\"\n",
    "        DESCR:\n",
    "        INPUT: remove_stopwords - bool - remove is, there, he etc...\n",
    "               remove_urls - bool - 't www.monkey.com t' --> 't com t'\n",
    "               remove_punct - bool - all punct and digits gone\n",
    "               lemmatize - bool - whether to apply lemmtization\n",
    "               extra_punct - str - other characters to remove\n",
    "               custom_stopwords - list - add to standard stops\n",
    "               custom_non_stopwords - list - make sure are kept\n",
    "               verbose - bool - whether to print progress statements\n",
    "               parser - str - 'big' or small, one keeps more, and is slower\n",
    "        OUTPUT: self - **due to other method, not this one\n",
    "        \"\"\"\n",
    "        # Initialize passed Attributes to specify operations\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_puncts = remove_puncts\n",
    "        self.lemmatize = lemmatize\n",
    "\n",
    "        # Change how operations work\n",
    "        self.custom_stopwords = custom_stopwords\n",
    "        self.custom_non_stopwords = custom_non_stopwords\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Set up punctation tranlation table\n",
    "        self.removals = string.punctuation + string.digits + extra_punct\n",
    "        self.trans_table = str.maketrans({key: None for key in self.removals})\n",
    "\n",
    "        #Load nlp model for parsing usage later\n",
    "        self.parser = spacy.load('en_core_web_sm', \n",
    "                                 disable=['parser','ner','textcat'])\n",
    "        #from spacy.lang.en import English\n",
    "        if parser == 'small':\n",
    "            self.parser = spacy.load('en')#English()\n",
    "\n",
    "        #Add custom stop words to nlp\n",
    "        for word in self.custom_stopwords:\n",
    "            self.parser.vocab[word].is_stop = True\n",
    "\n",
    "        #Set custom nlp words to be kept\n",
    "        for word in self.custom_non_stopwords:\n",
    "            self.parser.vocab[word].is_stop = False\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"take array of docs to clean array of docs\"\"\"\n",
    "        # Potential replace urls with tld ie www.monkey.com to com\n",
    "        if self.remove_urls:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"CHANGING URLS to TLDS...  \", end='')\n",
    "            X = [self.remove_url(doc) for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Potentially remove punctuation\n",
    "        if self.remove_puncts:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"REMOVING PUNCTUATION AND DIGITS... \", end='')\n",
    "            X = [doc.lower().translate(self.trans_table) for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Using Spacy to parse text\n",
    "        start_time = time()\n",
    "        if self.verbose:\n",
    "            print(\"PARSING TEXT WITH SPACY... \", end='')\n",
    "        #X = list(self.nlp.pipe(X))\n",
    "        X = list(self.parser.pipe(X))\n",
    "        if self.verbose:\n",
    "            print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Potential stopword removal\n",
    "        if self.remove_stopwords:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"REMOVING STOP WORDS FROM DOCUMENTS... \", end='')\n",
    "            X = [[word for word in doc if not word.is_stop] for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "\n",
    "        # Potential Lemmatization\n",
    "        if self.lemmatize:\n",
    "            start_time = time()\n",
    "            if self.verbose:\n",
    "                print(\"LEMMATIZING WORDS... \", end='')\n",
    "            X = [[word.lemma_ for word in doc] for doc in X]\n",
    "            if self.verbose:\n",
    "                print(f\"{time() - start_time:.0f} seconds\")\n",
    "\n",
    "        # Put back to normal if no lemmatizing happened\n",
    "        if not self.lemmatize:\n",
    "            X = [[str(word).lower() for word in doc] for doc in X]\n",
    "\n",
    "        # Join Back up\n",
    "        return [' '.join(lst) for lst in X]\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"interface conforming, and allows use of fit_transform\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(text):\n",
    "        \"\"\"\n",
    "        DESCR: given a url string find urls and replace with top level domain\n",
    "               a bit lazy in that if there are multiple all are replaced by first\n",
    "        INPUT: text - str - 'this is www.monky.com in text'\n",
    "        OUTPIT: str - 'this is <com> in text'\n",
    "        \"\"\"\n",
    "        # Define string to match urls\n",
    "        url_re = '((?:www|https?)(://)?[^\\s]+)'\n",
    "\n",
    "        # Find potential things to replace\n",
    "        matches = re.findall(url_re, text)\n",
    "        if matches == []:\n",
    "            return text\n",
    "\n",
    "        # Get tld of first match\n",
    "        match = matches[0][0]\n",
    "        try:\n",
    "            tld = get_tld(match, fail_silently=True, fix_protocol=True)\n",
    "        except ValueError:\n",
    "            tld = None\n",
    "\n",
    "        # failures return none so change to empty\n",
    "        if tld is None:\n",
    "            tld = \"\"\n",
    "\n",
    "        # make this obvsiouyly an odd tag\n",
    "        tld = f\"<{tld}>\"\n",
    "\n",
    "        # Make replacements and return\n",
    "        return re.sub(url_re, tld, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextNormalizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-c9f1cca8c9eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSklearnTopicModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-c9f1cca8c9eb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_topics, estimator)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         self.model = Pipeline([\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             ('tfidf', CountVectorizer(tokenizer=identity,\n\u001b[1;32m     35\u001b[0m                                       preprocessor=None, lowercase=False)),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextNormalizer' is not defined"
     ]
    }
   ],
   "source": [
    "#from reader import PickledCorpusReader\n",
    "#from transformers import TextNormalizer, GensimTfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from gensim.sklearn_api import lsimodel, ldamodel\n",
    "\n",
    "def identity(words):\n",
    "    return words\n",
    "\n",
    "\n",
    "class SklearnTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA',\n",
    "        To use Non-Negative Matrix Factorization, set estimator to 'NMF',\n",
    "        otherwise, defaults to Latent Dirichlet Allocation ('LDA').\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = TruncatedSVD(n_components=self.n_topics)\n",
    "        elif estimator == 'NMF':\n",
    "            self.estimator = NMF(n_components=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = LatentDirichletAllocation(n_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('tfidf', CountVectorizer(tokenizer=identity,\n",
    "                                      preprocessor=None, lowercase=False)),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        self.model.fit_transform(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def get_topics(self, n=25):\n",
    "        \"\"\"\n",
    "        n is the number of top terms to show for each topic\n",
    "        \"\"\"\n",
    "        vectorizer = self.model.named_steps['tfidf']\n",
    "        model = self.model.steps[-1][1]\n",
    "        names = vectorizer.get_feature_names()\n",
    "        topics = dict()\n",
    "\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            features = topic.argsort()[:-(n - 1): -1]\n",
    "            tokens = [names[i] for i in features]\n",
    "            topics[idx] = tokens\n",
    "\n",
    "        return topics\n",
    "\n",
    "\n",
    "df['processed_title'] = SklearnTopicModels().fit_transform(df.title)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(norm=None) #Ignoring the norm (ie. no Euclidean norm)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.get_shape())\n",
    "print(vectorizer.get_feature_names())\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PickledCorpusReader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-aee1bdeffb33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPickledCorpusReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPickledCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../corpus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PickledCorpusReader'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "import unicodedata\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.sklearn_api import lsimodel, ldamodel\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [\n",
    "            self.normalize(document)\n",
    "            for document in documents\n",
    "        ]\n",
    "\n",
    "\n",
    "class GensimTfidfVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, dirpath=\".\", tofull=False):\n",
    "        \"\"\"\n",
    "        Pass in a directory that holds the lexicon in corpus.dict and the\n",
    "        TFIDF model in tfidf.model (for now).\n",
    "\n",
    "        Set tofull = True if the next thing is a Scikit-Learn estimator\n",
    "        otherwise keep False if the next thing is a Gensim model.\n",
    "        \"\"\"\n",
    "        self._lexicon_path = os.path.join(dirpath, \"corpus.dict\")\n",
    "        self._tfidf_path = os.path.join(dirpath, \"tfidf.model\")\n",
    "\n",
    "        self.lexicon = None\n",
    "        self.tfidf = None\n",
    "        self.tofull = tofull\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        if os.path.exists(self._lexicon_path):\n",
    "            self.lexicon = Dictionary.load(self._lexicon_path)\n",
    "\n",
    "        if os.path.exists(self._tfidf_path):\n",
    "            self.tfidf = TfidfModel().load(self._tfidf_path)\n",
    "\n",
    "    def save(self):\n",
    "        self.lexicon.save(self._lexicon_path)\n",
    "        self.tfidf.save(self._tfidf_path)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        self.lexicon = Dictionary(documents)\n",
    "        self.tfidf = TfidfModel([self.lexicon.doc2bow(doc) for doc in documents], id2word=self.lexicon)\n",
    "        self.save()\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        def generator():\n",
    "            for document in documents:\n",
    "                vec = self.tfidf[self.lexicon.doc2bow(document)]\n",
    "                if self.tofull:\n",
    "                    yield sparse2full(vec)\n",
    "                else:\n",
    "                    yield vec\n",
    "        return list(generator())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from reader import PickledCorpusReader\n",
    "\n",
    "    corpus = PickledCorpusReader('../corpus')\n",
    "    docs = [\n",
    "        list(corpus.docs(fileids=fileid))[0]\n",
    "        for fileid in corpus.fileids()\n",
    "    ]\n",
    "\n",
    "    model = Pipeline([\n",
    "        ('norm', TextNormalizer()),\n",
    "        ('vect', GensimTfidfVectorizer()),\n",
    "        ('lda', ldamodel.LdaTransformer())])\n",
    "\n",
    "    model.fit_transform(docs)\n",
    "\n",
    "    print(model.named_steps['norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-0f0db8d87c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPickledCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../corpus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# With Sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "class GensimTopicModels(object):\n",
    "\n",
    "    def __init__(self, n_topics=50, estimator='LDA'):\n",
    "        \"\"\"\n",
    "        n_topics is the desired number of topics\n",
    "        To use Latent Semantic Analysis, set estimator to 'LSA'\n",
    "        otherwise defaults to Latent Dirichlet Allocation.\n",
    "        \"\"\"\n",
    "        self.n_topics = n_topics\n",
    "\n",
    "        if estimator == 'LSA':\n",
    "            self.estimator = lsimodel.LsiTransformer(num_topics=self.n_topics)\n",
    "        else:\n",
    "            self.estimator = ldamodel.LdaTransformer(num_topics=self.n_topics)\n",
    "\n",
    "        self.model = Pipeline([\n",
    "            ('norm', TextNormalizer()),\n",
    "            ('vect', GensimTfidfVectorizer()),\n",
    "            ('model', self.estimator)\n",
    "        ])\n",
    "\n",
    "    def fit(self, documents):\n",
    "        self.model.fit(documents)\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = PickledCorpusReader('../corpus')\n",
    "\n",
    "    # With Sklearn\n",
    "    skmodel = SklearnTopicModels(estimator='NMF')\n",
    "    documents   = corpus.docs()\n",
    "\n",
    "    skmodel.fit_transform(documents)\n",
    "    topics = skmodel.get_topics()\n",
    "    for topic, terms in topics.items():\n",
    "        print(\"Topic #{}:\".format(topic+1))\n",
    "        print(terms)\n",
    "\n",
    "    # # With Gensim\n",
    "    # gmodel = GensimTopicModels(estimator='LSA')\n",
    "    #\n",
    "    # docs = [\n",
    "    #     list(corpus.docs(fileids=fileid))[0]\n",
    "    #     for fileid in corpus.fileids()\n",
    "    # ]\n",
    "    #\n",
    "    # gmodel.fit(docs)\n",
    "    #\n",
    "    # # retrieve the fitted lsa model from the named steps of the pipeline\n",
    "    # lsa = gmodel.model.named_steps['lsa'].gensim_model\n",
    "    #\n",
    "    # # show the topics with the token-weights for the top 10 most influential tokens:\n",
    "    # print(lsa.print_topics(10))\n",
    "\n",
    "\n",
    "    # # retrieve the fitted lda model from the named steps of the pipeline\n",
    "    # lda = gmodel.model.named_steps['lda'].gensim_model\n",
    "    #\n",
    "    # # show the topics with the token-weights for the top 10 most influential tokens:\n",
    "    # lda.print_topics(10)\n",
    "\n",
    "    # corpus = [\n",
    "    #     gmodel.model.named_steps['vect'].lexicon.doc2bow(doc)\n",
    "    #     for doc in gmodel.model.named_steps['norm'].transform(docs)\n",
    "    # ]\n",
    "    #\n",
    "    #\n",
    "    # id2token = gmodel.model.named_steps['vect'].lexicon.id2token\n",
    "    #\n",
    "    # for word_id, freq in next(iter(corpus)):\n",
    "    #     print(id2token[word_id], freq)\n",
    "\n",
    "    # # get the highest weighted topic for each of the documents in the corpus\n",
    "    # def get_topics(vectorized_corpus, model):\n",
    "    #     from operator import itemgetter\n",
    "    #\n",
    "    #     topics = [\n",
    "    #         max(model[doc], key=itemgetter(1))[0]\n",
    "    #         for doc in vectorized_corpus\n",
    "    #     ]\n",
    "    #\n",
    "    #     return topics\n",
    "    #\n",
    "    # topics = get_topics(corpus,lda)\n",
    "    #\n",
    "    # for topic, doc in zip(topics, docs):\n",
    "    #     print(\"Topic:{}\".format(topic))\n",
    "    #     print(doc)\n",
    "    #\n",
    "    ## retreive the fitted vectorizer or the lexicon if needed\n",
    "    # tfidf = gmodel.model.named_steps['vect'].tfidf\n",
    "    # lexicon = gmodel.model.named_steps['vect'].lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
